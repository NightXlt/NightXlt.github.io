title: 卷积神经网络工作原理
date: 2019-2-15
tags: [Machine Learning]
categories: Machine Learning
description: 　　
---
## 概念
![enter description here](/images/CNN.PNG)
卷积神经网络即根据一张输入图片进行判断输出结果的中间黑箱。
![enter description here](/images/CNN-2.PNG)
无论其经过若干次基本处理，仍旧能识别出对应图像
## 提取原图特征——卷积
![enter description here](/images/feature-1.PNG)
以灰度图像为例亮的地方为1，暗的为-1.那么计算机是如何识别即使是旋转过的X呢？这就是卷积的操作了。
用卷积核在原图上滑动，进行卷积运算，得到特征图feature map。
卷积的本质：将原图中符合卷积核特征的特征提取出来，展示在feature map里面。卷积核进行卷积可以得到与卷积核特征一致的featuremap。
如果原图是X，卷积核是X，那么卷积核在原图上卷积运算之后生成的feature map也是X。
如果原图是O，卷积核是O，那么卷积核在原图上卷积运算之后生成的feature map也是O。
如果原图是O，卷积核是X，那么卷积核在原图上卷积运算之后生成的feature map就是乱码。
权值共享：卷积核扫过整张图片的过程中，卷积核参数不变。

下图的动画中，绿色表示原图像素值，红色数字表示卷积核中的参数，黄色表示卷积核在原图上滑动。右图表示卷积运算之后生成的feature map。
![enter description here](/images/feature-2.gif)
下图展示了RGB三个通道图片的卷积运算过程，共有两组卷积核，每组卷积核都有三个filter分别与原图的RGB三个通道进行卷积。每组卷积核各自生成一个feature map。
![enter description here](/images/feature-3.jpg)
> 原图最外圈补0：zero padding,便于提取图像边缘的特征

局部连接：feature map上每个值仅对应着原图的一小块区域，原图上的这块局部区域称作感受野（receptive field）。局部连接的思想，受启发于生物学里面的视觉系统，视觉皮层的神经元就是局部接受信息的。
![enter description here](/images/feature-4.jpg)
## 池化（下采样）：保留特征的同时压缩数据量
池化（Pooling）也叫做下采样（subsampling），用一个像素代替原图上邻近的若干像素，在保留feature map特征的同时压缩其大小。
池化的作用：
- 防止数据爆炸，节省运算量和运算时间。
- 防止过拟合、过学习。
池化可采取最大值和平均值来压缩大小
![enter description here](/images/pooling.jpg)

## 正则化：降低复杂模型的复杂度
在这里字符识别中采用relu激活函数max(0,x)进行抹零操作。因为矩阵中有较多的0可以方便运算。

## 重复
![enter description here](/images/layer.PNG)
将上述三个操作排列组合后重复若干次

## 全连接：最终通过加权计算得出结果
全连接层（可多层）：输出的每个神经元都和上一层每一个神经元连接。

卷积核的数量、大小、移动步长、补0的圈数是事先人为根据经验指定的，全连接层隐藏层的层数、神经元个数也是人为根据经验指定的（这叫做超参数），但其内部的参数是训练出来的。
![enter description here](/images/all-connection.PNG)
## 损失函数
计算神经网络的推测结果与图片真实标签的差距，构造损失函数，训练的目标就是将找到损失函数的最小值。
## 随机梯度下降
在减小损失函数的过程中，采用步步为营的方法，单个样本单个样本输入进行优化，而不是将全部样本计算之后再统一优化。虽然个别样本会出偏差，但随着样本数量增加，仍旧能够逐渐逼近损失函数最小值。
## 反向传播（Backpropagation）
通过求导找到损失函数的最小值，再一层一层反馈回去修改卷积核参数和全连接参数。